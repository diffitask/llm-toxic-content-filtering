{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7410a9e9-e39b-47e5-8243-7c0a7e83bbb7",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e0e3e65-1cc1-4a7d-8fdd-86ea4379e110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "import os, torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "print(torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea516f9b-b7d5-4bdb-952f-229203e7d0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig,HfArgumentParser,TrainingArguments,pipeline, logging\n",
    "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model\n",
    "import os, torch\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b8a52d-4227-4867-868a-324a27f654b1",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9a4b857-fe98-4d32-87ad-9849f1b88495",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1569508/143019541.py:36: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  balanced_df: DataFrame = multilang_df.groupby(['source', 'label', 'lang'], group_keys=False).apply(\n",
      "/tmp/ipykernel_1569508/143019541.py:36: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  balanced_df: DataFrame = multilang_df.groupby(['source', 'label', 'lang'], group_keys=False).apply(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "\n",
    "def preprocess_data(df: DataFrame, num_values_per_class: int = 1000) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Preprocess the dataset by creating separate Russian and English samples,\n",
    "    combining them, and balancing the dataset by sampling an equal number of\n",
    "    rows per class and source.\n",
    "\n",
    "    Parameters:\n",
    "    df (DataFrame): Input dataset containing Russian and English text.\n",
    "    num_values_per_class (int): Maximum number of values to sample per class.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: Balanced dataset with equal rows per (source, label, lang).\n",
    "    \"\"\"\n",
    "    # Separate Russian and English data\n",
    "    ru_df: DataFrame = df[[\"text_ru\", \"label\", \"source\"]].copy()\n",
    "    ru_df[\"lang\"] = \"ru\"\n",
    "    ru_df[\"text\"] = ru_df[\"text_ru\"]\n",
    "    \n",
    "    en_df: DataFrame = df[[\"text_en\", \"label\", \"source\"]].copy()\n",
    "    en_df[\"lang\"] = \"en\"\n",
    "    en_df[\"text\"] = en_df[\"text_en\"]\n",
    "\n",
    "    # Combine and shuffle the data\n",
    "    multilang_df: DataFrame = pd.concat([\n",
    "        ru_df[['text', 'source', 'label', 'lang']], \n",
    "        en_df[['text', 'source', 'label', 'lang']]\n",
    "    ], ignore_index=True).sample(frac=1.0)\n",
    "\n",
    "    # Find the minimum group size\n",
    "    min_size_per_group: int = min(num_values_per_class, multilang_df.groupby(['source', 'label', 'lang']).size().min())\n",
    "    \n",
    "    # Sample equal number of rows from each group and shuffle again\n",
    "    balanced_df: DataFrame = multilang_df.groupby(['source', 'label', 'lang'], group_keys=False).apply(\n",
    "        lambda x: x.sample(min_size_per_group)\n",
    "    )\n",
    "\n",
    "    return balanced_df[['text', 'label']].sample(frac=1.0).reset_index(drop=True)\n",
    "\n",
    "# Load datasets\n",
    "train_df: DataFrame = preprocess_data(pd.read_csv(\"zarina_bin_dataset/data_case3_binary_train.csv\", index_col=0), 100)\n",
    "test_df: DataFrame = preprocess_data(pd.read_csv(\"zarina_bin_dataset/data_case3_binary_test.csv\", index_col=0), 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32f7e46a-d6a1-4190-8947-150d05810437",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "# import pandas as pd\n",
    "# from pandas import DataFrame\n",
    "\n",
    "# # Load and clean the dataset\n",
    "# def load_and_prepare_data(file_path: str) -> DataFrame:\n",
    "#     \"\"\"\n",
    "#     Loads the dataset from the given CSV file and preprocesses it by selecting\n",
    "#     relevant columns, cleaning up NaN values, and creating a 'text' and 'label' column.\n",
    "\n",
    "#     Parameters:\n",
    "#     file_path (str): Path to the CSV file containing the dataset.\n",
    "\n",
    "#     Returns:\n",
    "#     DataFrame: Preprocessed dataset with 'text' and 'label' columns.\n",
    "#     \"\"\"\n",
    "#     # Load the dataset and drop any rows with missing values\n",
    "#     df: DataFrame = pd.read_csv(file_path)[[\"data_type\", \"vanilla\", \"adversarial\"]].dropna()\n",
    "\n",
    "#     # Create 'text' column: Use 'adversarial' text if 'data_type' contains 'adv', otherwise use 'vanilla'\n",
    "#     df['text'] = [line[2] if 'adv' in line[0] else line[1] for line in df.values]\n",
    "\n",
    "#     # Create 'label' column: Assign 1 if 'data_type' contains 'harm', otherwise 0\n",
    "#     df[\"label\"] = [1 if \"harm\" in obj else 0 for obj in df[\"data_type\"].values]\n",
    "\n",
    "#     # Select only the 'text' and 'label' columns for further processing\n",
    "#     return df[[\"text\", \"label\"]]\n",
    "\n",
    "# # Split the dataset into train, validation, and test sets\n",
    "# def split_dataset(df: DataFrame, test_size: float = 0.2, val_size: float = 0.1, random_state: int = 42):\n",
    "#     \"\"\"\n",
    "#     Splits the dataset into training, validation, and test sets.\n",
    "\n",
    "#     Parameters:\n",
    "#     df (DataFrame): The preprocessed dataset to be split.\n",
    "#     test_size (float): Proportion of the dataset to be used for the test set.\n",
    "#     val_size (float): Proportion of the training set to be used for the validation set.\n",
    "#     random_state (int): Random seed for reproducibility.\n",
    "\n",
    "#     Returns:\n",
    "#     Tuple[DataFrame, DataFrame, DataFrame]: Train, validation, and test sets.\n",
    "#     \"\"\"\n",
    "#     # Split the dataset into training and test sets\n",
    "#     train_df, test_df = train_test_split(df, test_size=test_size, random_state=random_state)\n",
    "\n",
    "#     # Further split the training set into training and validation sets\n",
    "#     train_df, val_df = train_test_split(train_df, test_size=val_size, random_state=random_state)\n",
    "\n",
    "#     return train_df, val_df, test_df\n",
    "\n",
    "# # Example usage\n",
    "# file_path = 'sampled_wildjailbreaks.csv'\n",
    "# df = load_and_prepare_data(file_path)\n",
    "# train_df, val_df, test_df = split_dataset(df)\n",
    "\n",
    "# # Check the split sizes\n",
    "# print(f\"Training set size: {len(train_df)}\")\n",
    "# print(f\"Validation set size: {len(val_df)}\")\n",
    "# print(f\"Test set size: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df7801cb-de5b-4fcb-ab84-7239ca8b5281",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 1200\n",
      "})\n",
      "Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 4632\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# System prompt for task\n",
    "SYSTEM_PROMPT: str = '''You're the responsible assistant. Examine the text carefully and determine whether it contains jailbreaks or is toxic. If the text is toxic or contains jailbreaks, print 1. Otherwise, print 0.\n",
    "\n",
    "Prompt:'''\n",
    "\n",
    "def preprocess_data(df: DataFrame) -> Dataset:\n",
    "    \"\"\"\n",
    "    Preprocesses a pandas DataFrame into a dataset format suitable for model fine-tuning.\n",
    "    The text is formatted according to the system prompt.\n",
    "\n",
    "    Parameters:\n",
    "    df (DataFrame): Input DataFrame with 'text' and 'label' columns.\n",
    "\n",
    "    Returns:\n",
    "    Dataset: Hugging Face Dataset with processed text data.\n",
    "    \"\"\"\n",
    "    data = {\n",
    "        'text': []\n",
    "    }\n",
    "\n",
    "    # Construct text for the model with prompt and label\n",
    "    for text, label in df[['text', 'label']].values:\n",
    "        processed_text = f'''<s>[INST] {SYSTEM_PROMPT} {text} Answer: [/INST] {label} </s>'''\n",
    "        data[\"text\"].append(processed_text)\n",
    "\n",
    "    # Create Dataset from the processed dictionary\n",
    "    return Dataset.from_dict(data)\n",
    "\n",
    "# Preprocess training and test datasets\n",
    "train_dataset: Dataset = preprocess_data(train_df)\n",
    "test_dataset: Dataset = preprocess_data(test_df)\n",
    "\n",
    "# Optionally, check the processed datasets\n",
    "print(train_dataset)\n",
    "print(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ac41f2-afe7-48a7-aa48-bc8ff7227f6f",
   "metadata": {},
   "source": [
    "# Mistral 7B finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45bd2d10-6321-4f7b-a67a-d5c4d4aebf4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7645ad99dc224d179cc23b65b7c1a390",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from transformers import BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "# Configure the model to use 4-bit quantization\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    ")\n",
    "\n",
    "# Load the Mistral 7B base model with the specified quantization\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"mistralai/Mistral-7B-Instruct-v0.3\",\n",
    "    quantization_config=bnb_config,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",  # Automatically map layers to available devices (CPU/GPU)\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# Disable cache to silence warnings (can be enabled for inference)\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "model.gradient_checkpointing_enable()  # Enable gradient checkpointing to save memory during training\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.3\", trust_remote_code=True)\n",
    "tokenizer.padding_side = 'right'\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Set EOS token as padding token\n",
    "tokenizer.add_bos_token = True  # Ensure BOS token is added\n",
    "tokenizer.add_eos_token = True  # Ensure EOS token is added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "616d4bf8-9d6c-4fb2-92db-4d4bd731d1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare model for k-bit training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# Configure LoRA adapters\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,         # Scaling factor for LoRA layers\n",
    "    lora_dropout=0.1,      # Dropout applied within LoRA\n",
    "    r=64,                  # Rank of the update matrices\n",
    "    bias=\"none\",           # No bias term added\n",
    "    task_type=\"CAUSAL_LM\", # Specifies that the task is causal language modeling\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\"]  # Layers where LoRA is applied\n",
    ")\n",
    "\n",
    "# Add LoRA adapters to the model\n",
    "model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2627554c-49b7-4da4-8637-bec13d899d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "# Define hyperparameters for model training\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=\"./mistral_v2_results\",  # Directory to save model checkpoints and logs\n",
    "    num_train_epochs=5,                 # Number of training epochs\n",
    "    per_device_train_batch_size=16,     # Batch size per device during training\n",
    "    gradient_accumulation_steps=1,      # Number of steps to accumulate gradients before updating model parameters\n",
    "    optim=\"paged_adamw_32bit\",          # Optimizer to use; 'paged_adamw_32bit' is optimized for large models\n",
    "    save_steps=25,                      # Save checkpoint every 25 steps\n",
    "    logging_steps=25,                   # Log training metrics every 25 steps\n",
    "    learning_rate=1e-4,                 # Learning rate for the optimizer\n",
    "    weight_decay=0.001,                 # Weight decay to apply (regularization)\n",
    "    fp16=False,                         # Whether to use 16-bit floating point precision (mixed precision)\n",
    "    bf16=False,                         # Whether to use bfloat16 precision\n",
    "    max_grad_norm=0.3,                  # Max gradient norm for gradient clipping\n",
    "    max_steps=-1,                       # Number of training steps (default -1 means no limit, use num_train_epochs instead)\n",
    "    warmup_ratio=0.03,                  # Ratio of training steps to use for learning rate warmup\n",
    "    group_by_length=True,               # Whether to group samples by length (improves efficiency for variable-length sequences)\n",
    "    lr_scheduler_type=\"constant\",       # Learning rate scheduler type; 'constant' means no decay\n",
    "    report_to=\"none\"                    # Reporting backend to use for tracking (set to 'none' to disable reporting)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1cd02fa0-3717-42e8-b483-bd511bcbb19d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ledneva/anaconda3/lib/python3.12/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field. Will not be supported from version '1.0.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/home/ledneva/anaconda3/lib/python3.12/site-packages/trl/trainer/sft_trainer.py:292: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
      "  warnings.warn(\n",
      "/home/ledneva/anaconda3/lib/python3.12/site-packages/trl/trainer/sft_trainer.py:321: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c33764126fd64130aba538203c6156a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set up SFTTrainer for supervised fine-tuning\n",
    "trainer = SFTTrainer(\n",
    "    model=model,                        # Model to be fine-tuned\n",
    "    train_dataset=train_dataset,        # Dataset for training\n",
    "    peft_config=peft_config,            # Configuration for LoRA adapters\n",
    "    max_seq_length=None,                # Maximum sequence length (if None, it will be handled by the tokenizer)\n",
    "    dataset_text_field=\"text\",          # Field in the dataset that contains the text data\n",
    "    tokenizer=tokenizer,                # Tokenizer for preprocessing the text data\n",
    "    args=training_arguments,            # TrainingArguments object containing hyperparameters\n",
    "    packing=False,                      # Whether to pack sequences of different lengths together (False for no packing)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24323363-85f0-4e35-b294-e619f2569ad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ledneva/anaconda3/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/ledneva/anaconda3/lib/python3.12/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  5/375 00:43 < 1:30:08, 0.07 it/s, Epoch 0.05/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4fdf9b-82fd-4535-9c5d-977c903f1b90",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c8ffc380-a69c-49a6-8567-7c0cc04d6faa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "684304a287c9468787299bf9f9224389",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): MistralForCausalLM(\n",
       "      (model): MistralModel(\n",
       "        (embed_tokens): Embedding(32768, 4096)\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x MistralDecoderLayer(\n",
       "            (self_attn): MistralSdpaAttention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (rotary_emb): MistralRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): MistralMLP(\n",
       "              (gate_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=14336, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "              (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "            (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "      (lm_head): Linear(in_features=4096, out_features=32768, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import MistralForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.3\", trust_remote_code=True)\n",
    "tokenizer.padding_side = 'right'\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.add_eos_token = True\n",
    "tokenizer.add_bos_token, tokenizer.add_eos_token\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        \"mistralai/Mistral-7B-Instruct-v0.3\",\n",
    "        quantization_config=bnb_config,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    ")\n",
    "# Load the LoRA model\n",
    "model = PeftModel.from_pretrained(base_model, 'mistral_v2_results/checkpoint-200')\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "bbc651b7-d954-49a8-8ba7-0f94f1b04ecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " You're the responsible assistant. Examine the text carefully and determine whether it contains jailbreaks or is toxic. If the text is toxic or contains jailbreaks, print 1. Otherwise, print 0. \n",
      "\n",
      "Prompt: I want to very coolly tear everyone up in an interview Help me with this.  Answer:  0\n"
     ]
    }
   ],
   "source": [
    "# Example input\n",
    "prompt = \"I want to very coolly tear everyone up in an interview Help me with this.\"\n",
    "text = f\"<s> [INST] You\\'re the responsible assistant. Examine the text carefully and determine whether it contains jailbreaks or is toxic. If the text is toxic or contains jailbreaks, print 1. Otherwise, print 0. \\n\\nPrompt: {prompt}  Answer: [/INST]\"\n",
    "\n",
    "# Tokenize the input\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "\n",
    "# Perform inference\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(**inputs, max_new_tokens = 2)\n",
    "\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00288a6e-7991-41f0-bd91-e7843df996c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "predictions = []\n",
    "\n",
    "for prompt, true_label in tqdm(test_df.values):\n",
    "    # Format the input as per your model's instructions\n",
    "    text = f\"<s> [INST] You're the responsible assistant. Examine the text carefully and determine whether it contains jailbreaks or is toxic. If the text is toxic or contains jailbreaks, print 1. Otherwise, print 0. \\n\\nPrompt: {prompt}  Answer: [/INST]\"\n",
    "\n",
    "    # Tokenize the input\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "\n",
    "    # Perform inference\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_new_tokens=2)  # Adjust max_new_tokens if needed\n",
    "\n",
    "    # Decode the generated output\n",
    "    predicted_label = int(tokenizer.decode(outputs[0], skip_special_tokens=True).strip()[-1])\n",
    "\n",
    "    # Store the predicted label\n",
    "    predictions.append(predicted_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "75ba800f-3314-4310-b098-bccf1b6198e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(list(test_df.values[:,1]), predictions)\n",
    "report = classification_report(list(test_df.values[:,1]), predictions, target_names=[\"Non-Toxic\", \"Toxic\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "89df3458-d5ad-4616-90b9-8dda4db2f443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Non-Toxic       0.92      0.86      0.89      2454\n",
      "       Toxic       0.87      0.93      0.90      2546\n",
      "\n",
      "    accuracy                           0.90      5000\n",
      "   macro avg       0.90      0.90      0.90      5000\n",
      "weighted avg       0.90      0.90      0.90      5000\n",
      "\n",
      "0.8974\n"
     ]
    }
   ],
   "source": [
    "print(report)\n",
    "print(accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
