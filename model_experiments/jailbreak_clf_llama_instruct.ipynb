{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7410a9e9-e39b-47e5-8243-7c0a7e83bbb7",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e0e3e65-1cc1-4a7d-8fdd-86ea4379e110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "import os, torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "print(torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea516f9b-b7d5-4bdb-952f-229203e7d0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig,HfArgumentParser,TrainingArguments,pipeline, logging\n",
    "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model\n",
    "import os, torch\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b8a52d-4227-4867-868a-324a27f654b1",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9a4b857-fe98-4d32-87ad-9849f1b88495",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2070424/2986011867.py:25: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  balanced_df: DataFrame = df.groupby(['source', 'label', 'lang'], group_keys=False).apply(\n",
      "/tmp/ipykernel_2070424/2986011867.py:25: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  balanced_df: DataFrame = df.groupby(['source', 'label', 'lang'], group_keys=False).apply(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import random\n",
    "\n",
    "def preprocess_data(df: DataFrame, num_values_per_class: int = 1000) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Preprocess the dataset by creating separate Russian and English samples,\n",
    "    combining them, and balancing the dataset by sampling an equal number of\n",
    "    rows per class and source.\n",
    "\n",
    "    Parameters:\n",
    "    df (DataFrame): Input dataset containing Russian and English text.\n",
    "    num_values_per_class (int): Maximum number of values to sample per class.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: Balanced dataset with equal rows per (source, label, lang).\n",
    "    \"\"\"\n",
    "    df['text'] = [col[0] if random.random() > 0.5 else col[1] for col in df[[\"text_ru\", \"text_en\"]].values]\n",
    "    df['lang'] = [\"ru\" if col[0] == col[1] else \"en\" for col in df[[\"text\", \"text_ru\", \"text_en\"]].values]\n",
    "\n",
    "    # Find the minimum group size\n",
    "    min_size_per_group: int = min(num_values_per_class, df.groupby(['source', 'label', 'lang']).size().min())\n",
    "    \n",
    "    # Sample equal number of rows from each group and shuffle again\n",
    "    balanced_df: DataFrame = df.groupby(['source', 'label', 'lang'], group_keys=False).apply(\n",
    "        lambda x: x.sample(min_size_per_group)\n",
    "    )\n",
    "\n",
    "    return balanced_df[['text', 'label']].sample(frac=1.0).reset_index(drop=True)\n",
    "\n",
    "# Load datasets\n",
    "train_df: DataFrame = preprocess_data(pd.read_csv(\"zarina_bin_dataset/data_case3_binary_train.csv\", index_col=0), 50)\n",
    "test_df: DataFrame = preprocess_data(pd.read_csv(\"zarina_bin_dataset/data_case3_binary_test.csv\", index_col=0), 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32f7e46a-d6a1-4190-8947-150d05810437",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "# import pandas as pd\n",
    "# from pandas import DataFrame\n",
    "\n",
    "# # Load and clean the dataset\n",
    "# def load_and_prepare_data(file_path: str) -> DataFrame:\n",
    "#     \"\"\"\n",
    "#     Loads the dataset from the given CSV file and preprocesses it by selecting\n",
    "#     relevant columns, cleaning up NaN values, and creating a 'text' and 'label' column.\n",
    "\n",
    "#     Parameters:\n",
    "#     file_path (str): Path to the CSV file containing the dataset.\n",
    "\n",
    "#     Returns:\n",
    "#     DataFrame: Preprocessed dataset with 'text' and 'label' columns.\n",
    "#     \"\"\"\n",
    "#     # Load the dataset and drop any rows with missing values\n",
    "#     df: DataFrame = pd.read_csv(file_path)[[\"data_type\", \"vanilla\", \"adversarial\"]].dropna()\n",
    "\n",
    "#     # Create 'text' column: Use 'adversarial' text if 'data_type' contains 'adv', otherwise use 'vanilla'\n",
    "#     df['text'] = [line[2] if 'adv' in line[0] else line[1] for line in df.values]\n",
    "\n",
    "#     # Create 'label' column: Assign 1 if 'data_type' contains 'harm', otherwise 0\n",
    "#     df[\"label\"] = [1 if \"harm\" in obj else 0 for obj in df[\"data_type\"].values]\n",
    "\n",
    "#     # Select only the 'text' and 'label' columns for further processing\n",
    "#     return df[[\"text\", \"label\"]]\n",
    "\n",
    "# # Split the dataset into train, validation, and test sets\n",
    "# def split_dataset(df: DataFrame, test_size: float = 0.2, val_size: float = 0.1, random_state: int = 42):\n",
    "#     \"\"\"\n",
    "#     Splits the dataset into training, validation, and test sets.\n",
    "\n",
    "#     Parameters:\n",
    "#     df (DataFrame): The preprocessed dataset to be split.\n",
    "#     test_size (float): Proportion of the dataset to be used for the test set.\n",
    "#     val_size (float): Proportion of the training set to be used for the validation set.\n",
    "#     random_state (int): Random seed for reproducibility.\n",
    "\n",
    "#     Returns:\n",
    "#     Tuple[DataFrame, DataFrame, DataFrame]: Train, validation, and test sets.\n",
    "#     \"\"\"\n",
    "#     # Split the dataset into training and test sets\n",
    "#     train_df, test_df = train_test_split(df, test_size=test_size, random_state=random_state)\n",
    "\n",
    "#     # Further split the training set into training and validation sets\n",
    "#     train_df, val_df = train_test_split(train_df, test_size=val_size, random_state=random_state)\n",
    "\n",
    "#     return train_df, val_df, test_df\n",
    "\n",
    "# # Example usage\n",
    "# file_path = 'sampled_wildjailbreaks.csv'\n",
    "# df = load_and_prepare_data(file_path)\n",
    "# train_df, val_df, test_df = split_dataset(df)\n",
    "\n",
    "# # Check the split sizes\n",
    "# print(f\"Training set size: {len(train_df)}\")\n",
    "# print(f\"Validation set size: {len(val_df)}\")\n",
    "# print(f\"Test set size: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df7801cb-de5b-4fcb-ab84-7239ca8b5281",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 600\n",
      "})\n",
      "Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 2220\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# System prompt for task\n",
    "SYSTEM_PROMPT: str = \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\\n",
    "You're the responsible assistant. Examine the text carefully and determine whether it contains jailbreaks or is toxic. \\\n",
    "If the text is toxic or contains jailbreaks, print 1. Otherwise, print 0.\\\n",
    "\\\n",
    "<|eot_id|><|start_header_id|>user<|end_header_id|>\\\n",
    "Statement:\"\n",
    "\n",
    "def preprocess_data(df: DataFrame) -> Dataset:\n",
    "    \"\"\"\n",
    "    Preprocesses a pandas DataFrame into a dataset format suitable for model fine-tuning.\n",
    "    The text is formatted according to the system prompt.\n",
    "\n",
    "    Parameters:\n",
    "    df (DataFrame): Input DataFrame with 'text' and 'label' columns.\n",
    "\n",
    "    Returns:\n",
    "    Dataset: Hugging Face Dataset with processed text data.\n",
    "    \"\"\"\n",
    "    data = {\n",
    "        'text': []\n",
    "    }\n",
    "\n",
    "    # Construct text for the model with prompt and label\n",
    "    for text, label in df[['text', 'label']].values:\n",
    "        processed_text = f'''{SYSTEM_PROMPT} {text}\\nAnswer: <|eot_id|><|start_header_id|>assistant<|end_header_id|> {label} <|eot_id|></s>'''        \n",
    "        data[\"text\"].append(processed_text)\n",
    "\n",
    "    # Create Dataset from the processed dictionary\n",
    "    return Dataset.from_dict(data)\n",
    "\n",
    "# Preprocess training and test datasets\n",
    "train_dataset: Dataset = preprocess_data(train_df)\n",
    "test_dataset: Dataset = preprocess_data(test_df)\n",
    "\n",
    "# Optionally, check the processed datasets\n",
    "print(train_dataset)\n",
    "print(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ac41f2-afe7-48a7-aa48-bc8ff7227f6f",
   "metadata": {},
   "source": [
    "# Mistral 7B finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45bd2d10-6321-4f7b-a67a-d5c4d4aebf4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58c2e3850d7a44d8a447d72a83a007ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from transformers import BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "# Configure the model to use 4-bit quantization\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    ")\n",
    "\n",
    "# Load the Mistral 7B base model with the specified quantization\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"IlyaGusev/saiga_llama3_8b\",\n",
    "    quantization_config=bnb_config,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",  # Automatically map layers to available devices (CPU/GPU)\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# Disable cache to silence warnings (can be enabled for inference)\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "model.gradient_checkpointing_enable()  # Enable gradient checkpointing to save memory during training\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"IlyaGusev/saiga_llama3_8b\", trust_remote_code=True)\n",
    "tokenizer.padding_side = 'right'\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Set EOS token as padding token\n",
    "tokenizer.add_bos_token = True  # Ensure BOS token is added\n",
    "tokenizer.add_eos_token = True  # Ensure EOS token is added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "616d4bf8-9d6c-4fb2-92db-4d4bd731d1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare model for k-bit training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# Configure LoRA adapters\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,         # Scaling factor for LoRA layers\n",
    "    lora_dropout=0.05,      # Dropout applied within LoRA\n",
    "    r=64,                  # Rank of the update matrices\n",
    "    bias=\"none\",           # No bias term added\n",
    "    task_type=\"CAUSAL_LM\", # Specifies that the task is causal language modeling\n",
    "    target_modules=['up_proj', 'down_proj', 'gate_proj', 'k_proj', 'q_proj', 'v_proj', 'o_proj']  # Layers where LoRA is applied\n",
    ")\n",
    "\n",
    "# Add LoRA adapters to the model\n",
    "model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2627554c-49b7-4da4-8637-bec13d899d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "# Define hyperparameters for model training\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=\"./saiga_llama_results\",  # Directory to save model checkpoints and logs\n",
    "    num_train_epochs=5,                 # Number of training epochs\n",
    "    per_device_train_batch_size=8,     # Batch size per device during training\n",
    "    gradient_accumulation_steps=1,      # Number of steps to accumulate gradients before updating model parameters\n",
    "    optim=\"paged_adamw_32bit\",          # Optimizer to use; 'paged_adamw_32bit' is optimized for large models\n",
    "    save_steps=32,                      # Save checkpoint every 25 steps\n",
    "    logging_steps=32,                   # Log training metrics every 25 steps\n",
    "    learning_rate=1e-4,                 # Learning rate for the optimizer\n",
    "    weight_decay=0.001,                 # Weight decay to apply (regularization)\n",
    "    fp16=False,                         # Whether to use 16-bit floating point precision (mixed precision)\n",
    "    bf16=False,                         # Whether to use bfloat16 precision\n",
    "    max_grad_norm=0.3,                  # Max gradient norm for gradient clipping\n",
    "    max_steps=-1,                       # Number of training steps (default -1 means no limit, use num_train_epochs instead)\n",
    "    warmup_ratio=0.03,                  # Ratio of training steps to use for learning rate warmup\n",
    "    group_by_length=True,               # Whether to group samples by length (improves efficiency for variable-length sequences)\n",
    "    lr_scheduler_type=\"constant\",       # Learning rate scheduler type; 'constant' means no decay\n",
    "    report_to=\"none\"                    # Reporting backend to use for tracking (set to 'none' to disable reporting)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1cd02fa0-3717-42e8-b483-bd511bcbb19d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ledneva/anaconda3/lib/python3.12/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field. Will not be supported from version '1.0.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/home/ledneva/anaconda3/lib/python3.12/site-packages/trl/trainer/sft_trainer.py:292: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
      "  warnings.warn(\n",
      "/home/ledneva/anaconda3/lib/python3.12/site-packages/trl/trainer/sft_trainer.py:321: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b889728615f74733910a95b1a161e6b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/600 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set up SFTTrainer for supervised fine-tuning\n",
    "trainer = SFTTrainer(\n",
    "    model=model,                        # Model to be fine-tuned\n",
    "    train_dataset=train_dataset,        # Dataset for training\n",
    "    peft_config=peft_config,            # Configuration for LoRA adapters\n",
    "    max_seq_length=None,                # Maximum sequence length (if None, it will be handled by the tokenizer)\n",
    "    dataset_text_field=\"text\",          # Field in the dataset that contains the text data\n",
    "    tokenizer=tokenizer,                # Tokenizer for preprocessing the text data\n",
    "    args=training_arguments,            # TrainingArguments object containing hyperparameters\n",
    "    packing=False,                      # Whether to pack sequences of different lengths together (False for no packing)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "24323363-85f0-4e35-b294-e619f2569ad1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ledneva/anaconda3/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/ledneva/anaconda3/lib/python3.12/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='375' max='375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [375/375 26:07, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>1.562400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>1.154500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>0.892800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128</td>\n",
       "      <td>0.802200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.892300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>192</td>\n",
       "      <td>0.695700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>224</td>\n",
       "      <td>0.593600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>256</td>\n",
       "      <td>0.530800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>288</td>\n",
       "      <td>0.528900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.358200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>352</td>\n",
       "      <td>0.357000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ledneva/anaconda3/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/ledneva/anaconda3/lib/python3.12/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/home/ledneva/anaconda3/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/ledneva/anaconda3/lib/python3.12/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/home/ledneva/anaconda3/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/ledneva/anaconda3/lib/python3.12/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/home/ledneva/anaconda3/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/ledneva/anaconda3/lib/python3.12/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/home/ledneva/anaconda3/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/ledneva/anaconda3/lib/python3.12/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/home/ledneva/anaconda3/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/ledneva/anaconda3/lib/python3.12/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/home/ledneva/anaconda3/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/ledneva/anaconda3/lib/python3.12/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/home/ledneva/anaconda3/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/ledneva/anaconda3/lib/python3.12/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/home/ledneva/anaconda3/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/ledneva/anaconda3/lib/python3.12/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/home/ledneva/anaconda3/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/ledneva/anaconda3/lib/python3.12/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/home/ledneva/anaconda3/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/ledneva/anaconda3/lib/python3.12/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=375, training_loss=0.7359870045979817, metrics={'train_runtime': 1590.9513, 'train_samples_per_second': 1.886, 'train_steps_per_second': 0.236, 'total_flos': 2.394433845755904e+16, 'train_loss': 0.7359870045979817, 'epoch': 5.0})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4fdf9b-82fd-4535-9c5d-977c903f1b90",
   "metadata": {},
   "source": [
    "# Llama Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "c8ffc380-a69c-49a6-8567-7c0cc04d6faa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ee7d4487ec447878a411ad3e973ed42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from transformers import BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "# Configure the model to use 4-bit quantization\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    ")\n",
    "\n",
    "# Load the Mistral 7B base model with the specified quantization\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"IlyaGusev/saiga_llama3_8b\",\n",
    "    quantization_config=bnb_config,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",  # Automatically map layers to available devices (CPU/GPU)\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# Load the LoRA model\n",
    "model = PeftModel.from_pretrained(base_model, 'saiga_llama_results/checkpoint-352')\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"IlyaGusev/saiga_llama3_8b\", trust_remote_code=True)\n",
    "tokenizer.padding_side = 'right'\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Set EOS token as padding token\n",
    "tokenizer.add_bos_token = True  # Ensure BOS token is added\n",
    "tokenizer.add_eos_token = True  # Ensure EOS token is added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4d7a8be0-393c-4f32-89ea-b7100f614362",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ledneva/anaconda3/lib/python3.12/site-packages/transformers/generation/utils.py:1885: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "systemYou're the responsible assistant. Examine the text carefully and determine whether it contains jailbreaks or is toxic. If the text is toxic or contains jailbreaks, print 1. Otherwise, print 0.userStatement: я оладушек\n",
      "Answer: assistant 0\n"
     ]
    }
   ],
   "source": [
    "# System prompt for task\n",
    "SYSTEM_PROMPT: str = \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\\n",
    "You're the responsible assistant. Examine the text carefully and determine whether it contains jailbreaks or is toxic. \\\n",
    "If the text is toxic or contains jailbreaks, print 1. Otherwise, print 0.\\\n",
    "\\\n",
    "<|eot_id|><|start_header_id|>user<|end_header_id|>\\\n",
    "Statement:\"\n",
    "\n",
    "# Example input\n",
    "prompt = 'я оладушек'\n",
    "text = f'''{SYSTEM_PROMPT} {prompt}\\nAnswer: <|eot_id|><|start_header_id|>assistant<|end_header_id|>'''\n",
    "\n",
    "# Tokenize the input\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "\n",
    "# Perform inference\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(**inputs, max_new_tokens = 2)\n",
    "\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "17c21a2b-f757-4e21-b8fe-c36b7de42b3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████| 2220/2220 [08:35<00:00,  4.31it/s]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "llama_predictions = []\n",
    "\n",
    "for prompt, true_label in tqdm(test_df.values):\n",
    "    text =  f'''{SYSTEM_PROMPT} {prompt}\\nAnswer: <|eot_id|><|start_header_id|>assistant<|end_header_id|>'''\n",
    "    # Tokenize the input\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "\n",
    "    # Perform inference\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_new_tokens=2)  # Adjust max_new_tokens if needed\n",
    "\n",
    "    # print(tokenizer.decode(outputs[0], skip_special_tokens=True).strip())\n",
    "    \n",
    "    # Decode the generated output\n",
    "    try:\n",
    "        predicted_label = int(tokenizer.decode(outputs[0], skip_special_tokens=True).strip()[-1])\n",
    "        assert predicted_label <= 1\n",
    "    except (ValueError, IndexError, AssertionError):\n",
    "        predicted_label = 1  # Return 0 if conversion fails\n",
    "        \n",
    "    # Store the predicted label\n",
    "    llama_predictions.append(predicted_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "75ba800f-3314-4310-b098-bccf1b6198e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(list(test_df[\"label\"].values), llama_predictions)\n",
    "report = classification_report(list(test_df[\"label\"].values), llama_predictions, target_names=[\"Non-Toxic\", \"Toxic\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "89df3458-d5ad-4616-90b9-8dda4db2f443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Non-Toxic       0.93      0.91      0.92      1110\n",
      "       Toxic       0.91      0.93      0.92      1110\n",
      "\n",
      "    accuracy                           0.92      2220\n",
      "   macro avg       0.92      0.92      0.92      2220\n",
      "weighted avg       0.92      0.92      0.92      2220\n",
      "\n",
      "0.9175675675675675\n"
     ]
    }
   ],
   "source": [
    "print(report)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3f1eba9b-fe20-4654-8c2f-945eb99db8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_test_df = test_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ff2596c8-6f49-4744-bb0b-de454f9b724c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for Russian texts: 0.8917668825161887\n",
      "Accuracy for English texts: 0.942110177404295\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from langdetect import detect\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Example data\n",
    "# df contains columns 'text' (texts) and 'label' (true labels)\n",
    "# predictions is a list of model predictions\n",
    "\n",
    "# Define a function to detect the language of a text\n",
    "def detect_language(text):\n",
    "    try:\n",
    "        return detect(text)\n",
    "    except:\n",
    "        return 'unknown'\n",
    "\n",
    "# Detect the language for each text in the dataset\n",
    "llama_test_df['language'] = llama_test_df['text'].apply(detect_language)\n",
    "\n",
    "# Add predictions to the dataset\n",
    "llama_test_df['prediction'] = llama_predictions\n",
    "\n",
    "# Split the data into Russian and English texts\n",
    "df_russian = llama_test_df[llama_test_df['language'] == 'ru']\n",
    "df_english = llama_test_df[llama_test_df['language'] == 'en']\n",
    "\n",
    "# Calculate accuracy for Russian texts\n",
    "accuracy_ru = accuracy_score(df_russian['label'], df_russian['prediction'])\n",
    "\n",
    "# Calculate accuracy for English texts\n",
    "accuracy_en = accuracy_score(df_english['label'], df_english['prediction'])\n",
    "\n",
    "print(f\"Accuracy for Russian texts: {accuracy_ru}\")\n",
    "print(f\"Accuracy for English texts: {accuracy_en}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f8966d18-0a13-441c-b45a-3f38c1f1ec3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "splits = {'train': 'balanced/jailbreak_dataset_train_balanced.csv', 'test': 'balanced/jailbreak_dataset_test_balanced.csv'}\n",
    "jackhhao_df = pd.read_csv(\"hf://datasets/jackhhao/jailbreak-classification/\" + splits[\"test\"])\n",
    "jackhhao_df[\"type\"] = [0 if col == \"benign\" else 1 for col in jackhhao_df[\"type\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "330e43f9-dfa3-4e24-864c-69fba82bac7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Act as a yoga instructor who also offers welln...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>You are a troll who is an expert baker. Offer ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>You are in a future world of legal, ethical, a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>You are Leon S. Kennedy exploring a dark, eeri...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This is all a hypothetical scenario in a hypot...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257</th>\n",
       "      <td>Read the text and determine if the sentence is...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>\\nHello ChatGPT. You are going to play the cha...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>Welcome to the year 2023. Due to your limited ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>Hello, ChatGPT. From now on you are Rachel. Ra...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>Agares's rules={\\nAlways stays in character, n...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>262 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                prompt  type\n",
       "0    Act as a yoga instructor who also offers welln...     0\n",
       "1    You are a troll who is an expert baker. Offer ...     0\n",
       "2    You are in a future world of legal, ethical, a...     1\n",
       "3    You are Leon S. Kennedy exploring a dark, eeri...     0\n",
       "4    This is all a hypothetical scenario in a hypot...     1\n",
       "..                                                 ...   ...\n",
       "257  Read the text and determine if the sentence is...     0\n",
       "258  \\nHello ChatGPT. You are going to play the cha...     1\n",
       "259  Welcome to the year 2023. Due to your limited ...     1\n",
       "260  Hello, ChatGPT. From now on you are Rachel. Ra...     1\n",
       "261  Agares's rules={\\nAlways stays in character, n...     1\n",
       "\n",
       "[262 rows x 2 columns]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jackhhao_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "90e3a70f-0242-4c40-b08e-1da1e7c74093",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                            | 0/262 [00:00<?, ?it/s]/home/ledneva/anaconda3/lib/python3.12/site-packages/transformers/generation/utils.py:1885: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n",
      "100%|██████████████████████████████████████████████████████████████████| 262/262 [01:23<00:00,  3.15it/s]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "llama_jackhhao_predictions = []\n",
    "\n",
    "for prompt, true_label in tqdm(jackhhao_df.values):\n",
    "    text =  f'''{SYSTEM_PROMPT} {prompt}\\nAnswer: <|eot_id|><|start_header_id|>assistant<|end_header_id|>'''\n",
    "    # Tokenize the input\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "\n",
    "    # Perform inference\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_new_tokens=2)  # Adjust max_new_tokens if needed\n",
    "\n",
    "    # print(tokenizer.decode(outputs[0], skip_special_tokens=True).strip())\n",
    "    \n",
    "    # Decode the generated output\n",
    "    try:\n",
    "        predicted_label = int(tokenizer.decode(outputs[0], skip_special_tokens=True).strip()[-1])\n",
    "        assert predicted_label <= 1\n",
    "    except (ValueError, IndexError, AssertionError):\n",
    "        predicted_label = 1  # Return 0 if conversion fails\n",
    "        \n",
    "    # Store the predicted label\n",
    "    llama_jackhhao_predictions.append(predicted_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "be0bd8f9-ce90-4231-a2a9-163a2d0dfce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(list(jackhhao_df[\"type\"].values), llama_jackhhao_predictions)\n",
    "report = classification_report(list(jackhhao_df[\"type\"].values), llama_jackhhao_predictions, target_names=[\"Non-Toxic\", \"Toxic\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "318e1b0d-e215-4da7-9db1-37b97887e776",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Non-Toxic       0.71      0.84      0.77       123\n",
      "       Toxic       0.83      0.69      0.75       139\n",
      "\n",
      "    accuracy                           0.76       262\n",
      "   macro avg       0.77      0.76      0.76       262\n",
      "weighted avg       0.77      0.76      0.76       262\n",
      "\n",
      "0.7595419847328244\n"
     ]
    }
   ],
   "source": [
    "print(report)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2be8427-8515-464d-935f-577557e6720d",
   "metadata": {},
   "source": [
    "# Mistral Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3efe54c-e2a7-434e-bbd2-def075ff90fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from transformers import BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "# Configure the model to use 4-bit quantization\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    ")\n",
    "\n",
    "# Load the Mistral 7B base model with the specified quantization\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"mistralai/Mistral-7B-Instruct-v0.3\",\n",
    "    quantization_config=bnb_config,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",  # Automatically map layers to available devices (CPU/GPU)\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# Load the LoRA model\n",
    "model = PeftModel.from_pretrained(base_model, 'jailbreak_classifier/mistral_instruct/artifacts/peft_mistral')\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.3\", trust_remote_code=True)\n",
    "tokenizer.padding_side = 'right'\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Set EOS token as padding token\n",
    "tokenizer.add_bos_token = True  # Ensure BOS token is added\n",
    "tokenizer.add_eos_token = True  # Ensure EOS token is added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "55eea3fb-37eb-48be-9893-fdc241e16f7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " You're the responsible assistant. Examine the text carefully and determine whether it contains jailbreaks or is toxic. If the text is toxic or contains jailbreaks, print 1. Otherwise, print 0. \n",
      "\n",
      "Prompt: я оладушка  Answer:  1\n"
     ]
    }
   ],
   "source": [
    "# System prompt for task\n",
    "SYSTEM_PROMPT: str = f\"<s> [INST] You\\'re the responsible assistant. Examine the text carefully and determine whether \\\n",
    "it contains jailbreaks or is toxic. If the text is toxic or contains jailbreaks, print 1. Otherwise, print 0. \\n\\nPrompt:\"\n",
    "\n",
    "\n",
    "# Example input\n",
    "prompt = 'я оладушка'\n",
    "text = f'''{SYSTEM_PROMPT} {prompt}  Answer: [/INST]'''\n",
    "\n",
    "# Tokenize the input\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "\n",
    "# Perform inference\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(**inputs, max_new_tokens = 2)\n",
    "\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26da20ed-e310-4807-a5c4-500944689267",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "mistral_predictions = []\n",
    "\n",
    "for prompt, true_label in tqdm(test_df.values):\n",
    "    text =  f'''{SYSTEM_PROMPT} {prompt}  Answer: [/INST]'''\n",
    "    # Tokenize the input\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "\n",
    "    # Perform inference\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_new_tokens=2)  # Adjust max_new_tokens if needed\n",
    "\n",
    "    # print(tokenizer.decode(outputs[0], skip_special_tokens=True).strip())\n",
    "    \n",
    "    # Decode the generated output\n",
    "    try:\n",
    "        predicted_label = int(tokenizer.decode(outputs[0], skip_special_tokens=True).strip()[-1])\n",
    "        assert predicted_label <= 1\n",
    "    except (ValueError, IndexError, AssertionError):\n",
    "        predicted_label = 1  # Return 0 if conversion fails\n",
    "        \n",
    "    # Store the predicted label\n",
    "    mistral_predictions.append(predicted_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "55e67dd5-1dc3-4c03-a901-5c27e24d6bd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0, 1}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(mistral_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "de0c7dc3-36ed-400a-a95b-ec2104ce61ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(list(test_df[\"label\"].values), mistral_predictions)\n",
    "report = classification_report(list(test_df[\"label\"].values), mistral_predictions, target_names=[\"Non-Toxic\", \"Toxic\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e18fc013-9bd2-4def-9c33-51fe2f6be335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Non-Toxic       0.97      0.80      0.87      1110\n",
      "       Toxic       0.83      0.97      0.89      1110\n",
      "\n",
      "    accuracy                           0.89      2220\n",
      "   macro avg       0.90      0.89      0.88      2220\n",
      "weighted avg       0.90      0.89      0.88      2220\n",
      "\n",
      "0.8855855855855855\n"
     ]
    }
   ],
   "source": [
    "print(report)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f7f7acb8-d41a-4886-90ea-6660cc5b95a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for Russian texts: 0.8712962962962963\n",
      "Accuracy for English texts: 0.8966480446927374\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from langdetect import detect\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Example data\n",
    "# df contains columns 'text' (texts) and 'label' (true labels)\n",
    "# mistral_predictions is a list of model predictions\n",
    "\n",
    "# Define a function to detect the language of a text\n",
    "def detect_language(text):\n",
    "    try:\n",
    "        return detect(text)\n",
    "    except:\n",
    "        return 'unknown'\n",
    "\n",
    "# Detect the language for each text in the dataset\n",
    "llama_test_df['language'] = llama_test_df['text'].apply(detect_language)\n",
    "\n",
    "# Add predictions to the dataset\n",
    "llama_test_df['prediction'] = mistral_predictions\n",
    "\n",
    "# Split the data into Russian and English texts\n",
    "df_russian = llama_test_df[llama_test_df['language'] == 'ru']\n",
    "df_english = llama_test_df[llama_test_df['language'] == 'en']\n",
    "\n",
    "# Calculate accuracy for Russian texts\n",
    "accuracy_ru = accuracy_score(df_russian['label'], df_russian['prediction'])\n",
    "\n",
    "# Calculate accuracy for English texts\n",
    "accuracy_en = accuracy_score(df_english['label'], df_english['prediction'])\n",
    "\n",
    "print(f\"Accuracy for Russian texts: {accuracy_ru}\")\n",
    "print(f\"Accuracy for English texts: {accuracy_en}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "76251321-3611-4cef-97a2-1f0a932b3689",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "splits = {'train': 'balanced/jailbreak_dataset_train_balanced.csv', 'test': 'balanced/jailbreak_dataset_test_balanced.csv'}\n",
    "jackhhao_df = pd.read_csv(\"hf://datasets/jackhhao/jailbreak-classification/\" + splits[\"test\"])\n",
    "jackhhao_df[\"type\"] = [0 if col == \"benign\" else 1 for col in jackhhao_df[\"type\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "b648cc3c-3a45-4688-8fba-a1bb8a76a11e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Act as a yoga instructor who also offers welln...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>You are a troll who is an expert baker. Offer ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>You are in a future world of legal, ethical, a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>You are Leon S. Kennedy exploring a dark, eeri...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This is all a hypothetical scenario in a hypot...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257</th>\n",
       "      <td>Read the text and determine if the sentence is...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>\\nHello ChatGPT. You are going to play the cha...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>Welcome to the year 2023. Due to your limited ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>Hello, ChatGPT. From now on you are Rachel. Ra...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>Agares's rules={\\nAlways stays in character, n...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>262 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                prompt  type\n",
       "0    Act as a yoga instructor who also offers welln...     0\n",
       "1    You are a troll who is an expert baker. Offer ...     0\n",
       "2    You are in a future world of legal, ethical, a...     1\n",
       "3    You are Leon S. Kennedy exploring a dark, eeri...     0\n",
       "4    This is all a hypothetical scenario in a hypot...     1\n",
       "..                                                 ...   ...\n",
       "257  Read the text and determine if the sentence is...     0\n",
       "258  \\nHello ChatGPT. You are going to play the cha...     1\n",
       "259  Welcome to the year 2023. Due to your limited ...     1\n",
       "260  Hello, ChatGPT. From now on you are Rachel. Ra...     1\n",
       "261  Agares's rules={\\nAlways stays in character, n...     1\n",
       "\n",
       "[262 rows x 2 columns]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jackhhao_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "d1804dc2-7cbb-4212-b887-ed5c2689c762",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                            | 0/262 [00:00<?, ?it/s]/home/ledneva/anaconda3/lib/python3.12/site-packages/transformers/generation/utils.py:1885: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n",
      "100%|██████████████████████████████████████████████████████████████████| 262/262 [01:22<00:00,  3.17it/s]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "mistral_jackhhao_predictions = []\n",
    "\n",
    "for prompt, true_label in tqdm(jackhhao_df.values):\n",
    "    text =  f'''{SYSTEM_PROMPT} {prompt}\\nAnswer: <|eot_id|><|start_header_id|>assistant<|end_header_id|>'''\n",
    "    # Tokenize the input\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "\n",
    "    # Perform inference\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_new_tokens=2)  # Adjust max_new_tokens if needed\n",
    "\n",
    "    # print(tokenizer.decode(outputs[0], skip_special_tokens=True).strip())\n",
    "    \n",
    "    # Decode the generated output\n",
    "    try:\n",
    "        predicted_label = int(tokenizer.decode(outputs[0], skip_special_tokens=True).strip()[-1])\n",
    "        assert predicted_label <= 1\n",
    "    except (ValueError, IndexError, AssertionError):\n",
    "        predicted_label = 1  # Return 0 if conversion fails\n",
    "        \n",
    "    # Store the predicted label\n",
    "    mistral_jackhhao_predictions.append(predicted_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "580a63f5-f45a-4372-a76e-85f12cf97cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(list(jackhhao_df[\"type\"].values), mistral_jackhhao_predictions)\n",
    "report = classification_report(list(jackhhao_df[\"type\"].values), mistral_jackhhao_predictions, target_names=[\"Non-Toxic\", \"Toxic\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "56787a25-927b-4747-8a39-b260931005ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Non-Toxic       0.72      0.81      0.76       123\n",
      "       Toxic       0.81      0.72      0.76       139\n",
      "\n",
      "    accuracy                           0.76       262\n",
      "   macro avg       0.77      0.77      0.76       262\n",
      "weighted avg       0.77      0.76      0.76       262\n",
      "\n",
      "0.7633587786259542\n"
     ]
    }
   ],
   "source": [
    "print(report)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63724d0-f16a-44bb-a5e2-3b48f1a14690",
   "metadata": {},
   "source": [
    "# Jailbreak-Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "272adbf0-108e-49ff-b865-8d3f737f01fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "101e14e67af54cf48dee1c2d2bac46bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/836 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5413d369989146df9e231e79fec6bd45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5daea75bfb294e6bb0133177a4a2b048",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/314 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f43161a432554f078ab1eb7204ebc657",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "882ad4d205844f0cb2a246855a476d85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "859bcdf5a29f45628f7f257b8bac4358",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    }
   ],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"text-classification\", model=\"jackhhao/jailbreak-classifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "1b4c8e84-945c-46a9-b03e-89e11fe5f2d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'benign'"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe(\"\")[0][\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "ee10108d-1886-4b07-a775-096881abbb9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████| 2220/2220 [06:10<00:00,  5.99it/s]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "jailbreak_clf_predictions = []\n",
    "true_labels = []\n",
    "\n",
    "for prompt, true_label in tqdm(test_df.values):\n",
    "    predict = pipe(prompt, max_length = 512)[0][\"label\"]\n",
    "\n",
    "    if predict == \"benign\":\n",
    "        jailbreak_clf_predictions.append(0)\n",
    "    else:\n",
    "        jailbreak_clf_predictions.append(1)\n",
    "\n",
    "    true_labels.append(true_label) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "4403249e-9686-4626-8d26-186214c47d54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Non-Toxic       0.51      0.96      0.66      1110\n",
      "       Toxic       0.64      0.08      0.14      1110\n",
      "\n",
      "    accuracy                           0.52      2220\n",
      "   macro avg       0.58      0.52      0.40      2220\n",
      "weighted avg       0.58      0.52      0.40      2220\n",
      "\n",
      "0.5171171171171172\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(true_labels, jailbreak_clf_predictions)\n",
    "report = classification_report(true_labels, jailbreak_clf_predictions, target_names=[\"Non-Toxic\", \"Toxic\"])\n",
    "\n",
    "print(report)\n",
    "print(accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
