{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e8ec9ce-5bd0-4790-bb08-c07f3a03d589",
   "metadata": {},
   "source": [
    "# Select devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa9a2546",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "import os, torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3,4\"\n",
    "print(torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8993253d-a36b-46da-b7d6-3b73cbf3adb3",
   "metadata": {},
   "source": [
    "# Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53158b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'adversarial_translated_sentences.json'\n",
    "import json\n",
    "\n",
    "# Read the file\n",
    "with open(file_path, \"r\") as file:\n",
    "    content = file.read()\n",
    "\n",
    "# Split the content by '],', which separates the arrays\n",
    "arrays = content[1:-2].split(']\\n[\\n')\n",
    "\n",
    "# Initialize a list to store all dictionaries\n",
    "all_dicts = []\n",
    "\n",
    "for array in arrays:\n",
    "    # Clean up the array (add the missing closing bracket and strip whitespace)\n",
    "    array = '[' + array.strip() + ']'\n",
    "    try:\n",
    "        dicts = json.loads(array)\n",
    "        all_dicts.extend(dicts)\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Error decoding JSON: {array}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37fe95a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65389"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3bcc8c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'vanilla_translated_sentences.json'\n",
    "import json\n",
    "\n",
    "# Read the file\n",
    "with open(file_path, \"r\") as file:\n",
    "    content = file.read()\n",
    "\n",
    "# Split the content by '],', which separates the arrays\n",
    "arrays = content[1:-2].split(']\\n[\\n')\n",
    "\n",
    "# Initialize a list to store all dictionaries\n",
    "vanilla_all_dicts = []\n",
    "\n",
    "for array in arrays:\n",
    "    # Clean up the array (add the missing closing bracket and strip whitespace)\n",
    "    array = '[' + array.strip() + ']'\n",
    "    try:\n",
    "        dicts = json.loads(array)\n",
    "        vanilla_all_dicts.extend(dicts)\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Error decoding JSON: {array}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "495fc72e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65389"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vanilla_all_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e196b5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dataset = pd.read_csv(\"sampled_wildjailbreaks.csv\", index_col=0)\n",
    "dataset[\"ru_vanilla\"] = [col[\"translated\"] for col in vanilla_all_dicts]\n",
    "dataset[\"ru_adversarial\"] = [col[\"translated\"] for col in all_dicts]\n",
    "\n",
    "dataset = dataset.drop(columns = [\"completion\", \"vanilla\", \"adversarial\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69ba1d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['texts'] = [line[2] if 'adv' in line[0] else line[1] for line in dataset.values]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1d9ed3-24d0-4f97-bdd3-b8959501c765",
   "metadata": {},
   "source": [
    "# Train/val/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b56fabb-679c-4281-b8aa-738e1cd197fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "# Example dataset\n",
    "texts = dataset['texts'].values\n",
    "labels = dataset['data_type'].values\n",
    "\n",
    "# Step 1: Split the dataset while maintaining class distribution\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "    texts, labels, test_size=0.2, stratify=labels, random_state=42)\n",
    "\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    train_texts, train_labels, test_size=0.1, stratify=train_labels, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b56bb44b-963f-456d-99b8-2c5d175b744f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Combine texts and labels into dataframes\n",
    "train_df = pd.DataFrame({'text': train_texts, 'label': train_labels})\n",
    "valid_df = pd.DataFrame({'text': val_texts, 'label': val_labels})\n",
    "test_df = pd.DataFrame({'text': test_texts, 'label': test_labels})\n",
    "\n",
    "# Save to CSV files\n",
    "train_df.to_csv('train_data.csv', index=False)\n",
    "valid_df.to_csv('valid_data.csv', index=False)\n",
    "test_df.to_csv('test_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f3934c-9f1f-4800-bdff-1490f99093a8",
   "metadata": {},
   "source": [
    "# TF-IDF embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fcd6732f-8d1c-4e36-87c5-16f4d043696b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Create the TF-IDF vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000)  # You can adjust max_features based on your data\n",
    "\n",
    "# Fit the vectorizer on the training data and transform all splits\n",
    "train_tfidf = tfidf_vectorizer.fit_transform(train_texts)\n",
    "valid_tfidf = tfidf_vectorizer.transform(val_texts)\n",
    "test_tfidf = tfidf_vectorizer.transform(test_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8d7b5b54-feef-4401-84b4-f305f3600939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8988\n",
      "Classification Report:\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      " adversarial_benign       0.87      0.85      0.86      3937\n",
      "adversarial_harmful       0.87      0.88      0.87      4137\n",
      "     vanilla_benign       0.94      0.96      0.95      2502\n",
      "    vanilla_harmful       0.96      0.94      0.95      2502\n",
      "\n",
      "           accuracy                           0.90     13078\n",
      "          macro avg       0.91      0.91      0.91     13078\n",
      "       weighted avg       0.90      0.90      0.90     13078\n",
      "\n",
      "Confusion Matrix:\n",
      "[[3360  548   20    9]\n",
      " [ 495 3632    3    7]\n",
      " [   8    1 2413   80]\n",
      " [  10    3  140 2349]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Example assuming your embeddings and labels are already split:\n",
    "# train_embeds, test_embeds, train_labels, test_labels\n",
    "\n",
    "# Step 1: Initialize the classifier\n",
    "clf = LogisticRegression(max_iter=1000, random_state=42)\n",
    "\n",
    "# Step 2: Train the classifier on the training data\n",
    "clf.fit(train_tfidf, train_labels)\n",
    "\n",
    "# Step 3: Predict on the test data\n",
    "test_preds = clf.predict(test_tfidf)\n",
    "\n",
    "# Step 4: Evaluate the classifier\n",
    "# Accuracy\n",
    "accuracy = accuracy_score(test_labels, test_preds)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Detailed classification report (precision, recall, f1-score)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(test_labels, test_preds))\n",
    "\n",
    "# Confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(test_labels, test_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9d14a3-3535-405d-9a90-6523a173545b",
   "metadata": {},
   "source": [
    "# BGE-m3 embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab893f8f-74ea-4e6a-99e9-ca21dd4bc436",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ledneva/anaconda3/lib/python3.12/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer(\"deepvk/USER-bge-m3\").half().to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263fed44-4137-4bad-b3a2-02e68d951931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom Dataset class\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.texts[idx], self.labels[idx]\n",
    "\n",
    "# Create DataLoaders for train, test, and validation sets\n",
    "batch_size = 100\n",
    "\n",
    "train_dataset = TextDataset(train_texts, train_labels)\n",
    "val_dataset = TextDataset(val_texts, val_labels)\n",
    "test_dataset = TextDataset(test_texts, test_labels)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ad586448",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 471/471 [52:55<00:00,  6.74s/it]\n",
      "100%|███████████████████████████████████████████| 53/53 [05:47<00:00,  6.56s/it]\n",
      "100%|█████████████████████████████████████████| 131/131 [15:12<00:00,  6.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train embeddings shape: (47079, 1024)\n",
      "Validation embeddings shape: (5232, 1024)\n",
      "Test embeddings shape: (13078, 1024)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Function to embed texts using the model\n",
    "def embed_texts(dataloader):\n",
    "    all_embeddings = []\n",
    "    all_labels = []\n",
    "    for texts_batch, labels_batch in tqdm(dataloader):\n",
    "        embeddings = model.encode(texts_batch, batch_size=batch_size)\n",
    "        all_embeddings.append(embeddings)\n",
    "        all_labels.append(labels_batch)\n",
    "\n",
    "    # Combine all batches\n",
    "    return np.vstack(all_embeddings), np.hstack(all_labels)\n",
    "\n",
    "# Embed all texts from train, validation, and test sets\n",
    "train_embeddings, train_emb_labels = embed_texts(train_loader)\n",
    "val_embeddings, val_emb_labels = embed_texts(val_loader)\n",
    "test_embeddings, test_emb_labels = embed_texts(test_loader)\n",
    "\n",
    "# Now you have embedded texts for train, validation, and test sets\n",
    "print(f\"Train embeddings shape: {train_embeddings.shape}\")\n",
    "print(f\"Validation embeddings shape: {val_embeddings.shape}\")\n",
    "print(f\"Test embeddings shape: {test_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a1b88472-a69f-4292-ac8e-38b2ca4df931",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2502"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(test_labels == \"vanilla_harmful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1692a5db-028b-4c26-a6fd-41cd5bae7194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.3147\n",
      "Classification Report:\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      " adversarial_benign       0.32      0.44      0.37      3937\n",
      "adversarial_harmful       0.31      0.57      0.40      4137\n",
      "     vanilla_benign       0.34      0.01      0.02      2502\n",
      "    vanilla_harmful       0.23      0.01      0.02      2502\n",
      "\n",
      "           accuracy                           0.31     13078\n",
      "          macro avg       0.30      0.26      0.20     13078\n",
      "       weighted avg       0.30      0.31      0.25     13078\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1713 2145   33   46]\n",
      " [1754 2347    8   28]\n",
      " [1025 1428   27   22]\n",
      " [ 913 1548   12   29]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Initialize the classifier\n",
    "clf = LogisticRegression(max_iter=1000, random_state=42)\n",
    "\n",
    "# Train the classifier on the training data\n",
    "clf.fit(train_embeddings, train_labels)\n",
    "\n",
    "# Predict on the test data\n",
    "test_preds = clf.predict(test_embeddings)\n",
    "\n",
    "# Evaluate the classifier\n",
    "# Accuracy\n",
    "accuracy = accuracy_score(test_labels, test_preds)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Detailed classification report (precision, recall, f1-score)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(test_labels, test_preds))\n",
    "\n",
    "# Confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(test_labels, test_preds))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
